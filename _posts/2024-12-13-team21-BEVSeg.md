---
layout: post
comments: true
title: Bird's Eye View Segmentation
author: Mihir Baviskar, Derek Jiang, Gabe Weisiger, Alan Michael
date: 2024-12-13
---


> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

---

## Introduction

## Motivation

## BeV Segmentation

## Lift, Splat, Shoot!

### Implementation

Lift, Splat, Shoot outlines a three-phased approach to autonomous driving. Of these, Lift and Splat deal with BeV segmentation, while Shoot uses these predictions to perform motion planning. The Lift step converts camera image data into spatial features using a combination of camera transforms and a CNN to learn depth. Afterwards, the Splat step flattens features into the BeV plane through pooling, then uses a U-Net architecture to generate the segmentation map.[1]

#### Lift

![Sample visualization]({{ '/assets/images/21/LiftSplatShoot_Depth.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. Visualization of the per-pixel predictions generated by the Lift model. The per-pixel context vector scaled with a "attention" vector, which describes the depths the context is most relevant to. [1]*

#### Splat

#### Splat

### Replication
We were able to replicate the results produced by Philion and Fidler from their provided Github Repository and model weights. On the nuscenes v1.0-mini dataset, the provided model achieved and IoU of 0.357 on a validation set of 81 samples, which is similar to the 33.03 given by the repository description, and the 32.07 IoU described in the paper. Inference on the validation samples took approximately 6.7 seconds on CPU.

#### Procedure
1. **Environment Setup** - As specified by the instructions provided on the [Github repository](https://github.com/nv-tlabs/lift-splat-shoot), we installed `nu-scenes-devkit`, `tensorboardX`, and `efficientnet_pytorch`. 
2. **Dataset** - The full nuscenes dataset was too large to download and set up on our local machine, so we downloaded nuscenes-mini, a subset of the data which with fewer scenes. 
3. **Run the model** - To run the model, we moved nuscenes to the correct location and downloaded pretrained model weights. Due to the age of the paper, the environment used Python 3.7, and we were unable to perform inference on GPU. 

In addition to the IoU benchmarks, authors also provided utilities for visualization, which we tested with success. On visualization, the six camera images is transformed into binary segmentation the BeV plane, with an additionalmap overlay from the nuscenes dataset. 

![Sample visualization]({{ '/assets/images/21/eval000002_000.jpg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. Visualization of Inference on nuscenes-mini*

## Additional Approaches

### PointBeV

#### Motivation

#### Architecure

#### Results

### BeVSegFormer

#### Motivation

BeVSegFormer seeks to improve upon current approaches to bird’s-eye-view semantic suggestions to help bolster its critical role within both autonomous driving and robot navigation systems. One of the core components which BeVSegFormer improves upon compared to other traditional approaches is the requirement of both intrinsic and extrinsic parameters of cameras which causes inaccuracies when dealing with distant objects or occlusions. Also, many of the current approaches rely on strict frameworks and camera setups that do not generalize well when changed. BeVSegFormer seeks to build a more flexible and scalable solution which not only has increased segmentation accuracy but also can support many different camera setups.[2] 

#### Architecture

BeVSegFormer utilizes a combination of many state of the art approaches. 

- Shared ResNet backbone
- BeV transformer encoder-decoder

![BeVSegFormer Network Archictecture]({{ '/assets/images/21/BVSegFormArch.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. BeVSegFormer Architecture (Image source: <https://arxiv.org/abs/2203.04050>)*

**Backbone:**
For one input image the ResNet backbone outputs a multi-scale feature map and for multiple camera inputs, the same backbone is shared and outputs corresponding feature maps. 
**Transformer Encoder-Decoder:**
The encoder utilizes multi-scale deformable self-attention that focuses on a set of sampling points near a reference point using the formula given below.

![Attention Formula]({{ '/assets/images/21/BEVSegFormul.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. Formula for Multi-Camera Deformable Self-Attention (Image source: <https://arxiv.org/abs/2203.04050>)*

Then, once encodes, the decoder computes cross-attention between the BEV queries and multi-camera feature maps. This approach adapts the DETR deformable attention module to a multicamera cross-attention module.

![BEV Transformer Decoder]({{ '/assets/images/21/BEVSegFormCross.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 3. BEV Transformer Decoder. (Image source: <https://arxiv.org/abs/2203.04050>)*

For each BEV query, a linear projection layer is used to predict the offset between sampling points and their corresponding reference points while sigmoid normalizing these coordinates. Then camera features from the sampled positions are collecting using the attention weights get a new query. 

The BeV semantic decoder reshapes the BeV queries into a 2D spatial features which are then fed into the two-stage BeV upsample module.

![BEV Semantic Decoder]({{ '/assets/images/21/BVSegFormSemDec.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 4. BeV Semantic Decoder Structure. (Image source: <https://arxiv.org/abs/2203.04050>)*

#### Results

Overall, BeVSegFormer produced promising results when compared to other when compared to other state of the art approaches that also do not use temporal information.

![BeVSegFormer Results]({{ '/assets/images/21/BVSegFormResu.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 5. Table with BEVSegFormer Results. (Image source: <https://arxiv.org/abs/2203.04050>)*

As seen above, BeVSegFormer actually outperformed the other existing methods by over ten percent in detecting a road divider, pedestrian crossing, road boundaries, and the overall IoU.

In training the BeVSegFormer, ablation was used to analyze which core components worked the best which shows how certain parts of the model boosted performance for certain tasks. 

![Ablation Table]({{ '/assets/images/21/BVSegFormAblation.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 6. Table with different ablation results. (Image source: <https://arxiv.org/abs/2203.04050>)*

Below we can see some of the segmentation output of the model from both only the front camera input and the use of multiple camera inputs surrounding the vehicle.

![Single Front Camera Model Segmentation Output]({{ '/assets/images/21/BVSegFormSingOut.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 7. Single Front Camera Model Segmentation Output. (Image source: <https://arxiv.org/abs/2203.04050>)*

![Multiple Camera Model Segmentation Output]({{ '/assets/images/21/BVSegFormMultOut.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 8. Multiple Camera Model Segmentation Output (Image source: <https://arxiv.org/abs/2203.04050>)*

## Societal Impact

## Conclusion

## References

[1]	J. Philion and S. Fidler, ‘Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D’, in Proceedings of the European Conference on Computer Vision, 2020.

[2] Peng, L., Chen, Z., Fu, Z., Liang, P., & Cheng, E. “BEVSegFormer: Bird’s Eye View Semantic Segmentation from Arbitrary Camera Rigs”. arXiv [cs.CV] 2022.
https://arxiv.org/abs/2203.04050 
References

---
