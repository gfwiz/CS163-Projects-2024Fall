---
layout: post
comments: true
title: Bird's Eye View Segmentation
author: Mihir Baviskar, Derek Jiang, Gabe Weisiger, Alan M 
date: 2024-12-13
---


> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

---

## Introduction

## Motivation

## BeV Segmentation

## Lift, Splat, Shoot!

### Implementation

### Testing

## Additionial Approaches

### PointBeV

#### Motivation

#### Architecure

#### Results

### BeVSegFormer

#### Motivation

BeVSegFormer seeks to improve upon current approaches to bird’s-eye-view semantic suggestions to help bolster its critical role within both autonomous driving and robot navigation systems. One of the core components which BeVSegFormer improves upon compared to other traditional approaches is the requirement of both intrinsic and extrinsic parameters of cameras which causes inaccuracies when dealing with distant objects or occlusions. Also, many of the current approaches rely on strict frameworks and camera setups that do not generalize well when changed. BeVSegFormer seeks to build a more flexible and scalable solution which not only has increased segmentation accuracy but also can support many different camera setups.[1] 

#### Architecture

BeVSegFormer utilizes a combination of many state of the art approaches. 

- Shared ResNet backbone
- BeV transformer encoder-decoder

![BeVSegFormer Network Archictecture]({{ '/assets/images/21/BVSegFormArch.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. BeVSegFormer Architecture (Image source: <https://arxiv.org/abs/2203.04050>)*

**Backbone:**
For one input image the ResNet backbone outputs a multi-scale feature map and for multiple camera inputs, the same backbone is shared and outputs corresponding feature maps. 
**Transformer Encoder-Decoder:**
The encoder utilizes multi-scale deformable self-attention that focuses on a set of sampling points near a reference point using the formula given below.

![Attention Formula]({{ '/assets/images/21/BEVSegFormul.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. Formula for Multi-Camera Deformable Self-Attention (Image source: <https://arxiv.org/abs/2203.04050>)*

Then, once encodes, the decoder computes cross-attention between the BEV queries and multi-camera feature maps. This approach adapts the DETR deformable attention module to a multicamera cross-attention module.

![BEV Transformer Decoder]({{ '/assets/images/21/BEVSegFormCross.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 3. BEV Transformer Decoder. (Image source: <https://arxiv.org/abs/2203.04050>)*

For each BEV query, a linear projection layer is used to predict the offset between sampling points and their corresponding reference points while sigmoid normalizing these coordinates. Then camera features from the sampled positions are collecting using the attention weights get a new query. 

The BeV semantic decoder reshapes the BeV queries into a 2D spatial features which are then fed into the two-stage BeV upsample module.

![BEV Semantic Decoder]({{ '/assets/images/21/BVSegFormSemDec.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 4. BeV Semantic Decoder Structure. (Image source: <https://arxiv.org/abs/2203.04050>)*

#### Results

Overall, BeVSegFormer produced promising results when compared to other when compared to other state of the art approaches that also do not use temporal information.

![BeVSegFormer Results]({{ '/assets/images/21/BVSegFormResu.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 5. Table with BEVSegFormer Results. (Image source: <https://arxiv.org/abs/2203.04050>)*

As seen above, BeVSegFormer actually outperformed the other existing methods by over ten percent in detecting a road divider, pedestrian crossing, road boundaries, and the overall IoU.

In training the BeVSegFormer, ablation was used to analyze which core components worked the best which shows how certain parts of the model boosted performance for certain tasks. 

![Ablation Table]({{ '/assets/images/21/BVSegFormAblation.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 6. Table with different ablation results. (Image source: <https://arxiv.org/abs/2203.04050>)*

Below we can see some of the segmentation output of the model from both only the front camera input and the use of multiple camera inputs surrounding the vehicle.

![Single Front Camera Model Segmentation Output]({{ '/assets/images/21/BVSegFormSingOut.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 7. Single Front Camera Model Segmentation Output. (Image source: <https://arxiv.org/abs/2203.04050>)*

![Multiple Camera Model Segmentation Output]({{ '/assets/images/21/BVSegFormMultOut.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 8. Multiple Camera Model Segmentation Output (Image source: <https://arxiv.org/abs/2203.04050>)*

## Societal Impact

## Conclusion

## References

[1] Peng, L., Chen, Z., Fu, Z., Liang, P., & Cheng, E. “BEVSegFormer: Bird’s Eye View Semantic Segmentation from Arbitrary Camera Rigs”. arXiv [cs.CV] 2022.
https://arxiv.org/abs/2203.04050 

---
