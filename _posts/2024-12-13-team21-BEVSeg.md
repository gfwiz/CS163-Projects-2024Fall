---
layout: post
comments: true
title: Bird's Eye View Segmentation
author: Mihir Baviskar, Derek Jiang, Gabe Weisiger, Alan Michael
date: 2024-12-13
---


> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

---

## Introduction
## Motivation:
Birds Eye View (BeV) Segmentation is crucial for the further development of self-driving cars and autonomous vehicles. Due to the nature of the self-driving cars, we need fast and reliable predictions that can utilize multiple car sensors and cameras. Since most cars need a full 360 understanding of their surroundings, BeV views provides a solution utilizing existing hardware by stitching together and generating a composite 360 image to run analysis on.
This article will explore the development of BeV segmentation using a Lift, Splat, Shoot architecture and the implementaiton and performance of such architecture. Additionally, though analyzing it’s benefits and overall impact relative to other solutions, we can estimate the limitations and drawbacks of this emerging technology.

## Image Segmentation:
In order to understand BeV segmentation, we must first understand Image segmentaiton is the process of dividing an image into multiple regions representing distinct objects. As an unsupervised labelling technique, image segmentation is similar to clustering pixels based on similar attributes in a high dimensional sapce. The process of segmentation usually has the input as a grid of pixels and generates segmentation masks for each pixel where we label it based on its class. 
Image Segmentation mainly relies on extracting features and many common techniques use grouping pixels based on similarity in texture, color, or intensity. The architecture we’re exploring consists of using Image Segmentation from a Birds Eye View context to classify outputs into regions.

### BeV Segmentation:
Differing from traditional Image Segmentation, Birds Eye View (BeV) Segmentation is the process of (1) translating multiple camera inputs into one rasterized BeV image output, (2) using segmentation algorithms to discriminate the inputted image based on regions, and (3) regenerate the output with segmented regions labeled. Beyond Image Segmentation, BeV Segmentation seeks to preserve 3 major properties:
* **Translation Equivariance**
	Continuous Transformations generated in the inputs should be preserved in the output. Basically, if the input is shifted the output should be proportionally shifted. This preserves spatial relationships and ensures our model is generating relevant features.
* **Permutation Invariance**
	Since the inputs are being aggregated to form the output, the order of the inputs should not affect the process of generating the outputs and the output itself. Without Permutation Invariance, our Segmentation algorithms may 
* **Ego-Frame Isometry Equivariance**

The approach to BeV we’ll be exploring is Lift, Splat, and Shoot. Where Lift is the process of extracting the features from the image and constructing a lower dimension feature space to run our algorithms on, Splat transposes the inputted images onto the BeV image coordinate system, and Shoot runs our segmentation algorithms on the newly constructed BeV representation of our input.

### Implementation

#### Lift:
By ensuring these properties are met we can still utilize traditional image segmentation techniques in our final Shoot stage and utilize transfer learning to bolster our model’s final performance. This architecture for BeV segmentation lifts the input data into feature space by converting from a 2D local dimension to a 3D frame. The 3D frame is shared across all cameras that way when extracted into features it maintains all properties and the features generated aren’t camera specific. 
However, when converting to feature space the individual point vectors to each pixel have an ambiguous depth. In order to solve this, the design generates representations within the 3D frame for all possible depths. At this point a large point cloud is created and the vectors corresponding to each point are then reparameterized to the new space. 
In order to resolve the depths we then decide to design a self-attention network and an initial depth inference. Through multiple training cycles, we can develop a function to query points in 3D space and generate the associated context vector.

#### Splat:
Lift generates a frustum shaped output for all imputed cameras and Splat takes these frustums and joins them in one singular BeV representation. By doing this in feature space, the model can significantly reduce parameters. Since BeV is traditionally used in self-driving cars, by splatting in feature space, we can reduce the amount of memory and time needed to generate predictions.
	Splat primarily uses a technique called pillar pooling. Traditional pillar pooling consists of aggregating data regions into localized pillars. In this case, we are taking the frustum point clouds and aggregating them into voxels of infinite height. These voxels are then vectorized and processed as tensors by the CNN during the shoot stage.

#### Shoot:
The Shoot phase of the architecture consists of using a Neural Motion Planner and running a cost-map learning on the inputted data. Neural Motion Planners are an architecture designed to predict a distribution over K template trajectories. In this case, a template trajectory is the path the predictor is moving transposed to relative opposite paths for all the objects in the BeV space. The final aspect of Shoot is running the actual segmentation algorithms, for Labels we use the L2 distance from these template trajectories and run the analysis to split the space into regions.


Lift, Splat, Shoot outlines a three-phased approach to autonomous driving. Of these, Lift and Splat deal with BeV segmentation, while Shoot uses these predictions to perform motion planning. The Lift step converts camera image data into spatial features using a combination of camera transforms and a CNN to learn depth. Afterwards, the Splat step flattens features into the BeV plane through pooling, then uses a U-Net architecture to generate the segmentation map.[1]

![Sample visualization]({{ '/assets/images/21/LiftSplatShoot_Depth.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. Visualization of the per-pixel predictions generated by the Lift model. The per-pixel context vector scaled with a "attention" vector, which describes the depths the context is most relevant to. [1]*

### Replication
We were able to replicate the results produced by Philion and Fidler from their provided Github Repository and model weights. On the nuscenes v1.0-mini dataset, the provided model achieved and IoU of 0.357 on a validation set of 81 samples, which is similar to the 33.03 given by the repository description, and the 32.07 IoU described in the paper. Inference on the validation samples took approximately 6.7 seconds on CPU.

#### Procedure
1. **Environment Setup** - As specified by the instructions provided on the [Github repository](https://github.com/nv-tlabs/lift-splat-shoot), we installed `nu-scenes-devkit`, `tensorboardX`, and `efficientnet_pytorch`. 
2. **Dataset** - The full nuscenes dataset was too large to download and set up on our local machine, so we downloaded nuscenes-mini, a subset of the data which with fewer scenes. 
3. **Run the model** - To run the model, we moved nuscenes to the correct location and downloaded pretrained model weights. Due to the age of the paper, the environment used Python 3.7, and we were unable to perform inference on GPU. 

In addition to the IoU benchmarks, authors also provided utilities for visualization, which we tested with success. On visualization, the six camera images is transformed into binary segmentation the BeV plane, with an additionalmap overlay from the nuscenes dataset. 

![Sample visualization]({{ '/assets/images/21/eval000002_000.jpg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. Visualization of Inference on nuscenes-mini*

## Additional Approaches

### PointBeV

#### Motivation

#### Architecure

#### Results

### BeVSegFormer

#### Motivation

BeVSegFormer seeks to improve upon current approaches to bird’s-eye-view semantic suggestions to help bolster its critical role within both autonomous driving and robot navigation systems. One of the core components which BeVSegFormer improves upon compared to other traditional approaches is the requirement of both intrinsic and extrinsic parameters of cameras which causes inaccuracies when dealing with distant objects or occlusions. Also, many of the current approaches rely on strict frameworks and camera setups that do not generalize well when changed. BeVSegFormer seeks to build a more flexible and scalable solution which not only has increased segmentation accuracy but also can support many different camera setups.[2] 

#### Architecture

BeVSegFormer utilizes a combination of many state of the art approaches. 

- Shared ResNet backbone
- BeV transformer encoder-decoder

![BeVSegFormer Network Archictecture]({{ '/assets/images/21/BVSegFormArch.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. BeVSegFormer Architecture (Image source: <https://arxiv.org/abs/2203.04050>)*

**Backbone:**
For one input image the ResNet backbone outputs a multi-scale feature map and for multiple camera inputs, the same backbone is shared and outputs corresponding feature maps. 
**Transformer Encoder-Decoder:**
The encoder utilizes multi-scale deformable self-attention that focuses on a set of sampling points near a reference point using the formula given below.

![Attention Formula]({{ '/assets/images/21/BEVSegFormul.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. Formula for Multi-Camera Deformable Self-Attention (Image source: <https://arxiv.org/abs/2203.04050>)*

Then, once encodes, the decoder computes cross-attention between the BEV queries and multi-camera feature maps. This approach adapts the DETR deformable attention module to a multicamera cross-attention module.

![BEV Transformer Decoder]({{ '/assets/images/21/BEVSegFormCross.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 3. BEV Transformer Decoder. (Image source: <https://arxiv.org/abs/2203.04050>)*

For each BEV query, a linear projection layer is used to predict the offset between sampling points and their corresponding reference points while sigmoid normalizing these coordinates. Then camera features from the sampled positions are collecting using the attention weights get a new query. 

The BeV semantic decoder reshapes the BeV queries into a 2D spatial features which are then fed into the two-stage BeV upsample module.

![BEV Semantic Decoder]({{ '/assets/images/21/BVSegFormSemDec.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 4. BeV Semantic Decoder Structure. (Image source: <https://arxiv.org/abs/2203.04050>)*

#### Results

Overall, BeVSegFormer produced promising results when compared to other when compared to other state of the art approaches that also do not use temporal information.

![BeVSegFormer Results]({{ '/assets/images/21/BVSegFormResu.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 5. Table with BEVSegFormer Results. (Image source: <https://arxiv.org/abs/2203.04050>)*

As seen above, BeVSegFormer actually outperformed the other existing methods by over ten percent in detecting a road divider, pedestrian crossing, road boundaries, and the overall IoU.

In training the BeVSegFormer, ablation was used to analyze which core components worked the best which shows how certain parts of the model boosted performance for certain tasks. 

![Ablation Table]({{ '/assets/images/21/BVSegFormAblation.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 6. Table with different ablation results. (Image source: <https://arxiv.org/abs/2203.04050>)*

Below we can see some of the segmentation output of the model from both only the front camera input and the use of multiple camera inputs surrounding the vehicle.

![Single Front Camera Model Segmentation Output]({{ '/assets/images/21/BVSegFormSingOut.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 7. Single Front Camera Model Segmentation Output. (Image source: <https://arxiv.org/abs/2203.04050>)*

![Multiple Camera Model Segmentation Output]({{ '/assets/images/21/BVSegFormMultOut.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 8. Multiple Camera Model Segmentation Output (Image source: <https://arxiv.org/abs/2203.04050>)*

## Societal Impact

## Conclusion

## References

[1]	J. Philion and S. Fidler, ‘Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D’, in Proceedings of the European Conference on Computer Vision, 2020.

[2] Peng, L., Chen, Z., Fu, Z., Liang, P., & Cheng, E. “BEVSegFormer: Bird’s Eye View Semantic Segmentation from Arbitrary Camera Rigs”. arXiv [cs.CV] 2022.
https://arxiv.org/abs/2203.04050 
References

---
